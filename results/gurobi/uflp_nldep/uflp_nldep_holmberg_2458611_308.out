solving instance uflp nldep holmberg 31 sqrt
Set parameter TokenServer to value "licences.gerad.lan"
Set parameter MIPGap to value 1e-06
Set parameter FuncPieceError to value 1e-06
Set parameter Threads to value 1
Set parameter FuncPieces to value -2
Set parameter TimeLimit to value 3600
Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (linux64 - "AlmaLinux 9.6 (Sage Margay)")

CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 56 physical cores, 56 logical processors, using up to 1 threads

Non-default parameters:
TimeLimit  3600
MIPGap  1e-06
Threads  1
FuncPieces  -2
FuncPieceError  1e-06

Optimize a model with 4740 rows, 4650 columns and 18150 nonzeros
Model fingerprint: 0x4cee3682
Model has 30 function constraints treated as nonlinear
  30 POW
Variable types: 4620 continuous, 30 integer (30 binary)
Coefficient statistics:
  Matrix range     [3e-04, 5e+02]
  Objective range  [1e+00, 3e+03]
  Bounds range     [3e-13, 1e+10]
  RHS range        [1e+00, 1e+00]
Warning: Model contains large bounds
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.

User MIP start did not produce a new incumbent solution

Presolve removed 4560 rows and 90 columns
Presolve time: 0.01s
Presolved: 330 rows, 4561 columns, 9330 nonzeros
Presolved model has 30 nonlinear constraint(s)

Solving non-convex MINLP

Variable types: 4561 continuous, 0 integer (0 binary)
Found heuristic solution: objective 14253.207228

Root relaxation: objective 8.389000e+03, 94 iterations, 0.00 seconds (0.00 work units)
Another try with MIP start

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 8389.00000    0   26 14253.2072 8389.00000  41.1%     -    0s
H    0     0                    10158.216438 8389.00000  17.4%     -    0s
     0     0 8390.29626    0   24 10158.2164 8390.29626  17.4%     -    0s
H    0     0                    10007.920568 8390.29626  16.2%     -    0s
H    0     0                    9953.8601796 8390.29626  15.7%     -    0s
     0     2 8390.33507    0   23 9953.86018 8390.33507  15.7%     -    0s
H    3     3                    9876.1235169 8411.17398  14.8%   3.7    0s
H    3     3                    9844.6223361 8411.17398  14.6%   3.7    0s
H    3     3                    9807.2189566 8411.17398  14.2%   3.7    0s
H  300   202                    9619.4261567 8495.57493  11.7%   4.1    1s
H  300   180                    9589.5729679 8495.57493  11.4%   4.1    1s
H  300   170                    9569.0256286 8495.57493  11.2%   4.1    1s
H  300   160                    9538.7644876 8495.57493  10.9%   4.1    1s
  NLP heuristic elapsed time = 5.02s
  NLP heuristic elapsed time = 10.03s
  NLP heuristic elapsed time = 15.01s
   639   348 9393.30324    9    0 9538.76449 8528.30605  10.6%   3.9   18s
  NLP heuristic elapsed time = 5.03s
  NLP heuristic elapsed time = 10.01s
  NLP heuristic elapsed time = 15.02s
   644   353 8626.27932   11    0 9538.76449 8528.30605  10.6%   4.0   34s
   646   354 8790.29338   14    0 9538.76449 8790.29338  7.85%   4.0   35s
  NLP heuristic elapsed time = 5.02s
  NLP heuristic elapsed time = 10.00s
  NLP heuristic elapsed time = 15.02s
   649   358 8790.29338   13    0 9538.76449 8790.29338  7.85%   4.2   51s
  NLP heuristic elapsed time = 5.03s
  NLP heuristic elapsed time = 10.02s
  NLP heuristic elapsed time = 15.05s
   654   363 9085.51778   23    0 9538.76449 8894.88421  6.75%   4.3   69s
H  656   345                    9530.3075549 8925.98054  6.34%   4.3   69s
  NLP heuristic elapsed time = 5.04s
  NLP heuristic elapsed time = 10.00s
  NLP heuristic elapsed time = 15.02s
   657   346 9229.03687   40    0 9530.30755 8925.98054  6.34%   4.3   88s
H  657   329                    9530.2533826 8935.81416  6.24%   4.5   88s
H  684   330                    9530.2472095 9021.90030  5.33%   4.5   88s
H  688   317                    9530.2400590 9021.90030  5.33%   4.4   88s
  1112   462 9259.59925   69   17 9530.24006 9046.04217  5.08%   4.1   90s
  3442  1721     cutoff   83      9530.24006 9188.30288  3.59%   3.4   95s
  5699  2774 9469.29307   72   16 9530.24006 9230.74654  3.14%   3.3  100s
  8081  3760     cutoff   77      9530.24006 9255.96028  2.88%   3.3  105s
 10332  4637     cutoff   76      9530.24006 9270.87357  2.72%   3.3  110s
 12684  5515 9472.93029   68    8 9530.24006 9283.70686  2.59%   3.3  115s
 14830  6305 9506.06557   74   16 9530.24006 9294.14624  2.48%   3.3  120s
 16962  7035     cutoff   71      9530.24006 9300.95942  2.41%   3.4  125s
 19285  7802 9425.72078   70   15 9530.24006 9311.59808  2.29%   3.4  130s
 21451  8484     cutoff   77      9530.24006 9317.48690  2.23%   3.4  135s
 23715  9154 9495.77223   73   13 9530.24006 9326.19191  2.14%   3.4  140s
 25873  9784 9494.60251   72   15 9530.24006 9331.38632  2.09%   3.4  145s
 28041 10392 9471.45225   72   16 9530.24006 9335.73616  2.04%   3.5  150s
 30303 10948     cutoff   76      9530.24006 9341.96427  1.98%   3.5  155s
 32588 11525 9484.54331   78   14 9530.24006 9346.98811  1.92%   3.5  160s
 34741 12082 9520.46122   76   11 9530.24006 9350.28697  1.89%   3.5  165s
 36877 12540     cutoff   77      9530.24006 9353.93780  1.85%   3.5  170s
 38950 13031     cutoff   80      9530.24006 9357.70487  1.81%   3.5  175s
 41060 13493 9434.08337   74   16 9530.24006 9360.69756  1.78%   3.5  180s
 43261 14044 9417.02100   70   16 9530.24006 9363.49722  1.75%   3.5  185s
 45392 14511 9415.85505   71   11 9530.24006 9366.25633  1.72%   3.6  190s
 47518 14945 9515.70090   79   19 9530.24006 9369.00679  1.69%   3.6  195s
 49766 15347 9467.06354   78   11 9530.24006 9372.07828  1.66%   3.6  200s
 51995 15826     cutoff   71      9530.24006 9374.59623  1.63%   3.6  205s
 54106 16181 9526.48010   74   11 9530.24006 9376.92979  1.61%   3.6  210s
 56347 16600     cutoff   78      9530.24006 9379.25354  1.58%   3.6  215s
 58592 17035     cutoff   84      9530.24006 9381.53692  1.56%   3.6  220s
 60688 17415 9477.11914   82   15 9530.24006 9383.41578  1.54%   3.6  225s
 62735 17712 9509.30966   74   14 9530.24006 9385.88555  1.51%   3.6  230s
 64824 18003 9443.63056   72   15 9530.24006 9387.80468  1.49%   3.6  235s
 66955 18362     cutoff   86      9530.24006 9389.78071  1.47%   3.6  240s
 68996 18679     cutoff   80      9530.24006 9391.16747  1.46%   3.6  245s
 71136 18981 9443.33784   69   11 9530.24006 9392.74687  1.44%   3.6  250s
 73272 19271 9503.25811   73   13 9530.24006 9394.50190  1.42%   3.6  255s
 75361 19608     cutoff   78      9530.24006 9396.17149  1.41%   3.6  260s
 77389 19872 9521.50227   74   15 9530.24006 9397.70839  1.39%   3.6  265s
 79396 20087 9461.30434   77   18 9530.24006 9399.38588  1.37%   3.7  270s
 81433 20318 9426.92022   73   14 9530.24006 9400.91761  1.36%   3.7  275s
 83531 20556 9475.02489   71   13 9530.24006 9402.16566  1.34%   3.7  280s
 85617 20776 9441.93609   76   15 9530.24006 9403.76055  1.33%   3.7  285s
 87700 20999     cutoff   78      9530.24006 9404.96178  1.31%   3.7  290s
 89659 21208 9525.07494   75    9 9530.24006 9406.12951  1.30%   3.7  295s
 91750 21451 9454.90123   70   13 9530.24006 9407.40267  1.29%   3.7  300s
 93827 21660 9511.05988   79   10 9530.24006 9408.42107  1.28%   3.7  305s
 95811 21878     cutoff   75      9530.24006 9409.59134  1.27%   3.7  310s
 97800 22055 9505.85564   71   11 9530.24006 9410.85673  1.25%   3.7  315s
 99891 22236 9480.08324   82   14 9530.24006 9412.03856  1.24%   3.7  320s
 101955 22442 9517.40396   73   15 9530.24006 9413.39050  1.23%   3.7  325s
 103902 22567 9443.14511   65    8 9530.24006 9414.58476  1.21%   3.7  330s
 105715 22662 9443.95100   68   16 9530.24006 9415.80932  1.20%   3.8  335s
 107729 22754     cutoff   73      9530.24006 9416.92609  1.19%   3.8  340s
 109640 22867 9464.76808   74   19 9530.24006 9418.03260  1.18%   3.8  345s
 111700 22989     cutoff   77      9530.24006 9419.01755  1.17%   3.8  350s
 113601 23056 9523.42011   69   10 9530.24006 9420.08353  1.16%   3.8  355s
 115575 23144 9494.25718   77   17 9530.24006 9421.13386  1.14%   3.8  360s
 117605 23252 9451.19566   67   12 9530.24006 9422.17535  1.13%   3.8  365s
 119546 23331 9498.02926   74   13 9530.24006 9423.07780  1.12%   3.8  370s
 121552 23441 9505.21733   77   12 9530.24006 9424.03053  1.11%   3.8  375s
 123574 23553     cutoff   75      9530.24006 9424.96799  1.10%   3.8  380s
 125495 23626     cutoff   73      9530.24006 9425.91264  1.09%   3.8  385s
 127427 23710 9492.73244   75   14 9530.24006 9426.87168  1.08%   3.9  390s
 129452 23745     cutoff   76      9530.24006 9427.80642  1.07%   3.9  395s
 131461 23820 9477.91223   67   13 9530.24006 9428.74839  1.06%   3.9  400s
 133442 23883     cutoff   78      9530.24006 9429.72407  1.05%   3.9  405s
 135445 23946 9476.04308   71   15 9530.24006 9430.58503  1.05%   3.9  410s
 137341 23988 9491.52994   73   14 9530.24006 9431.44535  1.04%   3.9  415s
 139210 24051     cutoff   84      9530.24006 9432.28319  1.03%   3.9  420s
 141209 24066     cutoff   79      9530.24006 9433.24442  1.02%   3.9  425s
 143217 24072 9455.04953   74   18 9530.24006 9434.16572  1.01%   3.9  430s
 145121 24084 9526.14663   82   12 9530.24006 9435.09196  1.00%   3.9  435s
 147010 24163 9511.86449   78   12 9530.24006 9435.89055  0.99%   3.9  440s
 148949 24202 9519.60602   76   11 9530.24006 9436.75014  0.98%   4.0  445s
 150922 24215 9518.55048   81   10 9530.24006 9437.64578  0.97%   4.0  450s
 152800 24213 9521.79790   86   13 9530.24006 9438.33853  0.96%   4.0  455s
 154685 24208 9465.04844   76   12 9530.24006 9439.03958  0.96%   4.0  460s
 156693 24182 9470.71356   79   14 9530.24006 9439.91450  0.95%   4.0  465s
 158657 24134 9466.76487   73   10 9530.24006 9440.75525  0.94%   4.0  470s
 160526 24137 9513.19387   81    8 9530.24006 9441.50953  0.93%   4.0  475s
 162512 24075     cutoff   74      9530.24006 9442.33681  0.92%   4.0  480s
 164490 24045     cutoff   83      9530.24006 9443.17848  0.91%   4.0  485s
 166454 24025 9472.60444   71   12 9530.24006 9444.04124  0.90%   4.0  490s
 168438 24005 9499.57687   73   11 9530.24006 9444.83525  0.90%   4.0  495s
 170441 23980 9471.73318   73   16 9530.24006 9445.72655  0.89%   4.0  500s
 172362 23945 9512.14368   72    8 9530.24006 9446.51782  0.88%   4.1  505s
 174368 23913 9511.36219   86   11 9530.24006 9447.36915  0.87%   4.1  510s
 176353 23922     cutoff   81      9530.24006 9448.25462  0.86%   4.1  515s
 178350 23877 9504.60662   76   15 9530.24006 9449.18436  0.85%   4.1  520s
 180334 23847 9480.20322   75   16 9530.24006 9450.12606  0.84%   4.1  525s
 182361 23740 9477.02165   72   12 9530.24006 9451.01833  0.83%   4.1  530s
 184353 23656 9515.40600   75   14 9530.24006 9451.93357  0.82%   4.1  535s
 186249 23636 9506.13538   77   11 9530.24006 9452.69027  0.81%   4.1  540s
 188229 23594     cutoff   80      9530.24006 9453.44919  0.81%   4.1  545s
 190212 23561 9503.05944   77   14 9530.24006 9454.34221  0.80%   4.1  550s
 192078 23453 9500.27263   72   13 9530.24006 9455.23773  0.79%   4.1  555s
 194080 23393     cutoff   74      9530.24006 9455.94954  0.78%   4.1  560s
 196087 23324     cutoff   79      9530.24006 9456.88332  0.77%   4.1  565s
 198002 23297 9514.95587   78   14 9530.24006 9457.57222  0.76%   4.1  570s
 200024 23189 9526.41874   74    8 9530.24006 9458.43731  0.75%   4.1  575s
 202003 23154 9503.14479   77   13 9530.24006 9459.25594  0.74%   4.1  580s
 203998 23063 9517.08414   77   13 9530.24006 9460.18905  0.74%   4.2  585s
 206035 22924 9471.73964   74   17 9530.24006 9461.04920  0.73%   4.2  590s
 207972 22773 9502.43946   79   10 9530.24006 9461.90399  0.72%   4.2  595s
 209959 22644     cutoff   75      9530.24006 9462.80182  0.71%   4.2  600s
 212000 22553     cutoff   75      9530.24006 9463.73067  0.70%   4.2  605s
 213972 22427 9497.63395   82   15 9530.24006 9464.60958  0.69%   4.2  610s
 216020 22255     cutoff   76      9530.24006 9465.51940  0.68%   4.2  615s
 217788 22189 9493.11565   74   15 9530.24006 9466.19689  0.67%   4.2  620s
 219865 22000 9503.12569   72   15 9530.24006 9466.98950  0.66%   4.2  625s
 221918 21725     cutoff   78      9530.24006 9467.98614  0.65%   4.2  630s
 223935 21594 9481.16569   74   14 9530.24006 9468.77689  0.64%   4.2  635s
 225957 21404     cutoff   79      9530.24006 9469.59836  0.64%   4.2  640s
 227975 21242 9486.37945   76   14 9530.24006 9470.35897  0.63%   4.2  645s
 230025 21052 9495.26867   77   13 9530.24006 9471.20599  0.62%   4.2  650s
 232123 20842 9497.83926   71   12 9530.24006 9471.98877  0.61%   4.2  655s
 234236 20589 9528.56744   81   10 9530.24006 9472.80272  0.60%   4.2  660s
 236327 20362     cutoff   76      9530.24006 9473.51224  0.60%   4.2  665s
 238386 20221     cutoff   77      9530.24006 9474.27036  0.59%   4.2  670s
 240484 20001 9510.06565   77   14 9530.24006 9475.04169  0.58%   4.2  675s
 242592 19719     cutoff   81      9530.24006 9475.99175  0.57%   4.2  680s
 244699 19404     cutoff   81      9530.24006 9476.88344  0.56%   4.2  685s
 246800 19093 9497.81184   74   15 9530.24006 9477.75629  0.55%   4.2  690s
 248810 18771 9482.35265   75   18 9530.24006 9478.56884  0.54%   4.2  695s
 250912 18449 9498.73647   80   13 9530.24006 9479.45561  0.53%   4.2  700s
 253037 18098     cutoff   72      9530.24006 9480.36530  0.52%   4.2  705s
 255169 17658     cutoff   79      9530.24006 9481.30666  0.51%   4.2  710s
 257316 17213 9494.68593   76   17 9530.24006 9482.29616  0.50%   4.2  715s
 259470 16683     cutoff   73      9530.24006 9483.45583  0.49%   4.2  720s
 261670 16087 9510.43758   74   16 9530.24006 9484.67810  0.48%   4.2  725s
 263778 15447     cutoff   81      9530.24006 9485.84156  0.47%   4.3  730s
 265867 14852 9523.19827   74    9 9530.24006 9486.95056  0.45%   4.3  735s
 267757 14294 9494.16580   72   15 9530.24006 9487.97260  0.44%   4.3  740s
 269867 13546 9497.17969   75   17 9530.24006 9489.13560  0.43%   4.3  745s
 271903 12814 9508.84079   73   15 9530.24006 9490.31978  0.42%   4.3  750s
 273976 11973     cutoff   81      9530.24006 9491.54923  0.41%   4.3  755s
 276073 11140     cutoff   78      9530.24006 9492.76419  0.39%   4.3  760s
 278166 10157     cutoff   81      9530.24006 9494.37712  0.38%   4.3  765s
 280015  9350     cutoff   75      9530.24006 9495.92244  0.36%   4.3  770s
 281977  8404 9504.06206   74   13 9530.24006 9497.86943  0.34%   4.3  775s
 283934  7459     cutoff   82      9530.24006 9499.96550  0.32%   4.3  780s
 285879  6658     cutoff   76      9530.24006 9502.31022  0.29%   4.3  785s
 287789  5888     cutoff   81      9530.24006 9504.48719  0.27%   4.3  790s
 289756  4877     cutoff   86      9530.24006 9507.67836  0.24%   4.3  795s
 291622  3851     cutoff   86      9530.24006 9511.20964  0.20%   4.3  800s
 293635  2564 9530.22888   83    9 9530.24006 9515.81757  0.15%   4.4  805s
 295624  1239     cutoff   84      9530.24006 9521.73851  0.09%   4.4  810s
 297696   351     cutoff  104      9530.24006 9528.54391  0.02%   4.4  815s
*298130   199             108    9530.2387908 9529.49201  0.01%   4.4  816s

Explored 298500 nodes (1312341 simplex iterations) in 817.23 seconds (285.48 work units)
Thread count was 1 (of 56 available processors)

Solution count 10: 9530.24 9530.25 9538.76 ... 9953.86

Optimal solution found (tolerance 1.00e-06)
Best objective 9.530238790845e+03, best bound 9.530238790845e+03, gap 0.0000%

User-callback calls 1086195, time in user-callback 0.59 sec
reading holmberg data file ../../../../../instances/uflp/holmberg/p31
primal_bound = 9530.238790845357, dual_bound = 9530.238790845355, gap = 1.9086503953008587e-14, time = 817.24
OPTIMAL LOCATIONS:
y[1] = 1.0
y[2] = 1.0
y[3] = 1.0
y[4] = 1.0
y[5] = 1.0
y[6] = 1.0
y[7] = 1.0
y[8] = 1.0
y[9] = 1.0
y[10] = 1.0
y[11] = 1.0
y[12] = 1.0
y[13] = 1.0
y[14] = 1.0
y[15] = 1.0
y[16] = 1.0
y[17] = 1.0
y[18] = 1.0
y[19] = 1.0
y[20] = 1.0
y[21] = 1.0
y[22] = 1.0
y[23] = 1.0
y[24] = 1.0
y[25] = 1.0
y[26] = 1.0
y[27] = 1.0
y[28] = 1.0
y[29] = 1.0
y[30] = 1.0
OPTIMAL WAREHOUSING:
f[1] = 146.0
f[3] = 187.0
f[6] = 335.0
f[9] = 242.0
f[12] = 433.0
f[15] = 329.0
f[16] = 20.0
f[18] = 399.0
f[21] = 451.0
f[25] = 428.0
OPTIMAL WAREHOUSING COSTS:
nlobj[1] = 110.8574840481079
nlobj[2] = 0.0
nlobj[3] = 125.46205312138179
nlobj[4] = 0.0
nlobj[5] = 0.0
nlobj[6] = 167.92447007574026
nlobj[7] = 0.0
nlobj[8] = 0.0
nlobj[9] = 142.72474081158893
nlobj[10] = 0.0
nlobj[11] = 0.0
nlobj[12] = 190.91290113971365
nlobj[13] = 0.0
nlobj[14] = 0.0
nlobj[15] = 166.41394910221183
nlobj[16] = 41.03044818794584
nlobj[17] = 0.0
nlobj[18] = 183.26439459213486
nlobj[19] = 0.0
nlobj[20] = 0.0
nlobj[21] = 194.84084072501656
nlobj[22] = 0.0
nlobj[23] = 0.0
nlobj[24] = 0.0
nlobj[25] = 189.80750904151392
nlobj[26] = 0.0
nlobj[27] = 0.0
nlobj[28] = 0.0
nlobj[29] = 0.0
nlobj[30] = 0.0
OPTIMAL ASSIGNMENTS:
x[1, 22] = 1.0
x[1, 41] = 1.0
x[1, 61] = 1.0
x[1, 68] = 1.0
x[1, 82] = 1.0
x[1, 126] = 1.0
x[1, 129] = 1.0
x[1, 131] = 1.0
x[1, 141] = 1.0
x[3, 7] = 1.0
x[3, 21] = 1.0
x[3, 27] = 1.0
x[3, 91] = 1.0
x[3, 97] = 1.0
x[3, 105] = 1.0
x[3, 113] = 1.0
x[3, 149] = 1.0
x[6, 6] = 1.0
x[6, 19] = 1.0
x[6, 23] = 1.0
x[6, 25] = 1.0
x[6, 29] = 1.0
x[6, 31] = 1.0
x[6, 51] = 1.0
x[6, 54] = 1.0
x[6, 62] = 1.0
x[6, 90] = 1.0
x[6, 106] = 1.0
x[6, 121] = 1.0
x[6, 127] = 1.0
x[6, 134] = 1.0
x[6, 138] = 1.0
x[6, 148] = 1.0
x[6, 150] = 1.0
x[9, 12] = 1.0
x[9, 13] = 1.0
x[9, 26] = 1.0
x[9, 44] = 1.0
x[9, 50] = 1.0
x[9, 76] = 1.0
x[9, 101] = 1.0
x[9, 110] = 1.0
x[9, 122] = 1.0
x[9, 125] = 1.0
x[9, 135] = 1.0
x[9, 142] = 1.0
x[9, 145] = 1.0
x[12, 2] = 1.0
x[12, 20] = 1.0
x[12, 32] = 1.0
x[12, 34] = 1.0
x[12, 39] = 1.0
x[12, 49] = 1.0
x[12, 58] = 1.0
x[12, 59] = 1.0
x[12, 64] = 1.0
x[12, 65] = 1.0
x[12, 66] = 1.0
x[12, 73] = 1.0
x[12, 94] = 1.0
x[12, 99] = 1.0
x[12, 104] = 1.0
x[12, 108] = 1.0
x[12, 114] = 1.0
x[12, 115] = 1.0
x[12, 116] = 1.0
x[12, 123] = 1.0
x[12, 130] = 1.0
x[12, 132] = 1.0
x[12, 146] = 1.0
x[12, 147] = 1.0
x[15, 4] = 1.0
x[15, 9] = 1.0
x[15, 16] = 1.0
x[15, 17] = 1.0
x[15, 40] = 1.0
x[15, 42] = 1.0
x[15, 46] = 1.0
x[15, 52] = 1.0
x[15, 53] = 1.0
x[15, 60] = 1.0
x[15, 79] = 1.0
x[15, 109] = 1.0
x[15, 111] = 1.0
x[15, 112] = 1.0
x[15, 136] = 1.0
x[15, 144] = 1.0
x[16, 43] = 1.0
x[18, 10] = 1.0
x[18, 15] = 1.0
x[18, 18] = 1.0
x[18, 24] = 1.0
x[18, 47] = 1.0
x[18, 57] = 1.0
x[18, 74] = 1.0
x[18, 84] = 1.0
x[18, 85] = 1.0
x[18, 86] = 1.0
x[18, 87] = 1.0
x[18, 88] = 1.0
x[18, 93] = 1.0
x[18, 95] = 1.0
x[18, 98] = 1.0
x[18, 102] = 1.0
x[18, 103] = 1.0
x[18, 107] = 1.0
x[18, 137] = 1.0
x[18, 140] = 1.0
x[21, 1] = 1.0
x[21, 3] = 1.0
x[21, 5] = 1.0
x[21, 14] = 1.0
x[21, 33] = 1.0
x[21, 35] = 1.0
x[21, 36] = 1.0
x[21, 48] = 1.0
x[21, 55] = 1.0
x[21, 56] = 1.0
x[21, 63] = 1.0
x[21, 71] = 1.0
x[21, 72] = 1.0
x[21, 78] = 1.0
x[21, 81] = 1.0
x[21, 83] = 1.0
x[21, 89] = 1.0
x[21, 100] = 1.0
x[21, 124] = 1.0
x[21, 139] = 1.0
x[21, 143] = 1.0
x[25, 8] = 1.0
x[25, 11] = 1.0
x[25, 28] = 1.0
x[25, 30] = 1.0
x[25, 37] = 1.0
x[25, 38] = 1.0
x[25, 45] = 1.0
x[25, 67] = 1.0
x[25, 69] = 1.0
x[25, 70] = 1.0
x[25, 75] = 1.0
x[25, 77] = 1.0
x[25, 80] = 1.0
x[25, 92] = 1.0
x[25, 96] = 1.0
x[25, 117] = 1.0
x[25, 118] = 1.0
x[25, 119] = 1.0
x[25, 120] = 1.0
x[25, 128] = 1.0
x[25, 133] = 1.0
fixed costs = 15000.0
assignment costs = 8017.0
warehousing costs = 1513.2387908453554
total cost = 9530.238790845355
