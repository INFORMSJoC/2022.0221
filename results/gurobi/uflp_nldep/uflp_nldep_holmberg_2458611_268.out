solving instance uflp nldep holmberg 27 sqrt
Set parameter TokenServer to value "licences.gerad.lan"
Set parameter MIPGap to value 1e-06
Set parameter FuncPieceError to value 1e-06
Set parameter Threads to value 1
Set parameter FuncPieces to value -2
Set parameter TimeLimit to value 3600
Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (linux64 - "AlmaLinux 9.6 (Sage Margay)")

CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 56 physical cores, 56 logical processors, using up to 1 threads

Non-default parameters:
TimeLimit  3600
MIPGap  1e-06
Threads  1
FuncPieces  -2
FuncPieceError  1e-06

Optimize a model with 4740 rows, 4650 columns and 18150 nonzeros
Model fingerprint: 0x4cee3682
Model has 30 function constraints treated as nonlinear
  30 POW
Variable types: 4620 continuous, 30 integer (30 binary)
Coefficient statistics:
  Matrix range     [3e-04, 5e+02]
  Objective range  [1e+00, 3e+03]
  Bounds range     [3e-13, 1e+10]
  RHS range        [1e+00, 1e+00]
Warning: Model contains large bounds
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.

User MIP start did not produce a new incumbent solution

Presolve removed 4560 rows and 90 columns
Presolve time: 0.01s
Presolved: 330 rows, 4561 columns, 9330 nonzeros
Presolved model has 30 nonlinear constraint(s)

Solving non-convex MINLP

Variable types: 4561 continuous, 0 integer (0 binary)
Found heuristic solution: objective 14253.207228

Root relaxation: objective 8.389000e+03, 94 iterations, 0.00 seconds (0.00 work units)
Another try with MIP start

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 8389.00000    0   26 14253.2072 8389.00000  41.1%     -    0s
H    0     0                    10158.216438 8389.00000  17.4%     -    0s
     0     0 8390.29626    0   24 10158.2164 8390.29626  17.4%     -    0s
H    0     0                    10007.920568 8390.29626  16.2%     -    0s
H    0     0                    9953.8601796 8390.29626  15.7%     -    0s
     0     2 8390.33507    0   23 9953.86018 8390.33507  15.7%     -    0s
H    3     3                    9876.1235169 8411.17398  14.8%   3.7    0s
H    3     3                    9844.6223361 8411.17398  14.6%   3.7    0s
H    3     3                    9807.2189566 8411.17398  14.2%   3.7    0s
H  300   202                    9619.4261567 8495.57493  11.7%   4.1    1s
H  300   180                    9589.5729679 8495.57493  11.4%   4.1    1s
H  300   170                    9569.0256286 8495.57493  11.2%   4.1    1s
H  300   160                    9538.7644876 8495.57493  10.9%   4.1    1s
  NLP heuristic elapsed time = 5.04s
  NLP heuristic elapsed time = 10.02s
  NLP heuristic elapsed time = 15.01s
   639   348 9393.30324    9    0 9538.76449 8528.30605  10.6%   3.9   18s
  NLP heuristic elapsed time = 5.04s
  NLP heuristic elapsed time = 10.03s
  NLP heuristic elapsed time = 15.03s
   644   353 8626.27932   11    0 9538.76449 8528.30605  10.6%   4.0   35s
  NLP heuristic elapsed time = 5.00s
  NLP heuristic elapsed time = 10.01s
  NLP heuristic elapsed time = 15.02s
   649   358 8790.29338   13    0 9538.76449 8790.29338  7.85%   4.2   51s
  NLP heuristic elapsed time = 5.01s
  NLP heuristic elapsed time = 10.00s
  NLP heuristic elapsed time = 15.04s
   654   363 9085.51778   23    0 9538.76449 8894.88421  6.75%   4.3   68s
H  656   345                    9530.3075549 8925.98054  6.34%   4.3   69s
  NLP heuristic elapsed time = 5.02s
  NLP heuristic elapsed time = 10.03s
  NLP heuristic elapsed time = 15.02s
  NLP heuristic elapsed time = 20.03s
   657   346 9229.03687   40    0 9530.30755 8925.98054  6.34%   4.3   89s
H  657   329                    9530.2533826 8935.81416  6.24%   4.5   89s
H  684   330                    9530.2472095 9021.90030  5.33%   4.5   89s
H  688   317                    9530.2400590 9021.90030  5.33%   4.4   89s
   785   360 9287.64625   65   15 9530.24006 9025.10016  5.30%   4.4   90s
  2905  1466 9311.01670   66   15 9530.24006 9175.12067  3.73%   3.3   95s
  5063  2524 9336.24265   66   15 9530.24006 9225.29559  3.20%   3.3  100s
  7326  3439 9365.47138   65   15 9530.24006 9247.76166  2.96%   3.3  105s
  9489  4334     cutoff   81      9530.24006 9266.31713  2.77%   3.3  110s
 11749  5194 9454.27371   73   18 9530.24006 9279.06238  2.64%   3.3  115s
 13768  5967 9514.68840   76   16 9530.24006 9290.41360  2.52%   3.3  120s
 15794  6651 9510.62978   76   17 9530.24006 9296.76947  2.45%   3.4  125s
 17995  7362     cutoff   79      9530.24006 9306.38599  2.35%   3.4  130s
 20109  8056 9482.55531   69   15 9530.24006 9313.98117  2.27%   3.4  135s
 22271  8718 9462.93121   71   15 9530.24006 9320.32973  2.20%   3.4  140s
 24443  9376     cutoff   80      9530.24006 9328.06361  2.12%   3.4  145s
 26523  9958 9456.10187   71   18 9530.24006 9332.61159  2.07%   3.4  150s
 28591 10524 9511.38127   70   12 9530.24006 9336.99972  2.03%   3.5  155s
 30761 11038 9490.21609   75   19 9530.24006 9343.45582  1.96%   3.5  160s
 32927 11588 9375.10900   73   16 9530.24006 9347.41703  1.92%   3.5  165s
 34999 12134 9450.67141   71   16 9530.24006 9350.63558  1.88%   3.5  170s
 37060 12585 9429.84727   69   17 9530.24006 9354.43030  1.84%   3.5  175s
 39065 13052 9474.28531   76   17 9530.24006 9357.96345  1.81%   3.5  180s
 41093 13512 9519.03843   80   17 9530.24006 9360.69756  1.78%   3.5  185s
 43225 14042 9488.15988   75   15 9530.24006 9363.47192  1.75%   3.5  190s
 45274 14487 9503.82642   79   12 9530.24006 9366.12821  1.72%   3.6  195s
 47333 14912 9506.31915   77   19 9530.24006 9368.62034  1.70%   3.6  200s
 49515 15286 9519.68928   80   15 9530.24006 9371.80677  1.66%   3.6  205s
 51648 15749 9450.67648   74   18 9530.24006 9374.27098  1.64%   3.6  210s
 53639 16128     cutoff   75      9530.24006 9376.39062  1.61%   3.6  215s
 55745 16504 9455.57692   76   17 9530.24006 9378.73497  1.59%   3.6  220s
 57858 16895 9458.79537   71   17 9530.24006 9380.65006  1.57%   3.6  225s
 59972 17269 9508.03973   81   12 9530.24006 9382.86531  1.55%   3.6  230s
 61938 17607 9478.75934   73   11 9530.24006 9384.92257  1.52%   3.6  235s
 63913 17898 9500.19839   67    9 9530.24006 9386.98111  1.50%   3.6  240s
 66025 18210 9519.95097   72   10 9530.24006 9389.00262  1.48%   3.6  245s
 68008 18533 9418.77463   69   16 9530.24006 9390.54095  1.47%   3.6  250s
 70074 18843 9461.47267   72   17 9530.24006 9392.02613  1.45%   3.6  255s
 72148 19107     cutoff   77      9530.24006 9393.72215  1.43%   3.6  260s
 74194 19435 9520.50832   80   16 9530.24006 9395.28376  1.42%   3.6  265s
 76167 19712     cutoff   78      9530.24006 9396.85574  1.40%   3.6  270s
 78228 19961 9521.00793   80    9 9530.24006 9398.39079  1.38%   3.7  275s
 80165 20186 9510.12750   71   14 9530.24006 9400.10906  1.37%   3.7  280s
 82094 20399 9500.34406   73   11 9530.24006 9401.29760  1.35%   3.7  285s
 84130 20621 9486.19392   77   15 9530.24006 9402.57785  1.34%   3.7  290s
 86057 20826 9468.59572   71   14 9530.24006 9404.05349  1.32%   3.7  295s
 88072 21047 9519.86402   83   11 9530.24006 9405.23987  1.31%   3.7  300s
 89981 21244 9477.26206   78   12 9530.24006 9406.31074  1.30%   3.7  305s
 92002 21471 9489.93580   76   13 9530.24006 9407.52495  1.29%   3.7  310s
 94007 21676 9528.72800   74   10 9530.24006 9408.57390  1.28%   3.7  315s
 95920 21883 9516.62738   69   14 9530.24006 9409.63495  1.27%   3.7  320s
 97856 22063     cutoff   76      9530.24006 9410.87350  1.25%   3.7  325s
 99891 22236 9480.08324   82   14 9530.24006 9412.03856  1.24%   3.7  330s
 101918 22441 9515.79257   74   10 9530.24006 9413.36007  1.23%   3.7  335s
 103823 22560     cutoff   79      9530.24006 9414.55603  1.21%   3.7  340s
 105600 22649     cutoff   72      9530.24006 9415.76890  1.20%   3.8  345s
 107578 22743 9507.55743   81   14 9530.24006 9416.84823  1.19%   3.8  350s
 109439 22856 9523.38397   74   14 9530.24006 9417.91065  1.18%   3.8  355s
 111424 22985     cutoff   83      9530.24006 9418.81872  1.17%   3.8  360s
 113267 23046 9522.22599   75   12 9530.24006 9419.90894  1.16%   3.8  365s
 115139 23116 9483.20709   71   15 9530.24006 9420.89598  1.15%   3.8  370s
 117143 23224     cutoff   80      9530.24006 9421.93390  1.14%   3.8  375s
 119011 23296 9498.32110   65   11 9530.24006 9422.80803  1.13%   3.8  380s
 120981 23422 9489.62885   74   15 9530.24006 9423.79400  1.12%   3.8  385s
 122958 23527 9514.32417   74   12 9530.24006 9424.70063  1.11%   3.8  390s
 124819 23600 9497.37482   81   14 9530.24006 9425.51775  1.10%   3.8  395s
 126673 23686 9495.96937   76   16 9530.24006 9426.58415  1.09%   3.9  400s
 128618 23749 9501.87434   78   11 9530.24006 9427.43365  1.08%   3.9  405s
 130556 23781 9487.58251   76   11 9530.24006 9428.45206  1.07%   3.9  410s
 132506 23859 9455.26309   74   14 9530.24006 9429.26388  1.06%   3.9  415s
 134449 23914     cutoff   80      9530.24006 9430.10428  1.05%   3.9  420s
 136277 23960 9524.65471   81   12 9530.24006 9430.97193  1.04%   3.9  425s
 138201 24022 9443.97377   75   15 9530.24006 9431.80705  1.03%   3.9  430s
 140013 24062 9485.26973   72   16 9530.24006 9432.68311  1.02%   3.9  435s
 141951 24082 9459.49662   74   17 9530.24006 9433.49492  1.02%   3.9  440s
 143775 24080 9463.04715   69   11 9530.24006 9434.48464  1.00%   3.9  445s
 145687 24126 9463.85452   74   11 9530.24006 9435.30451  1.00%   3.9  450s
 147564 24169     cutoff   77      9530.24006 9436.15644  0.99%   4.0  455s
 149502 24201 9476.15024   69   13 9530.24006 9437.03635  0.98%   4.0  460s
 151391 24200 9498.49713   74    8 9530.24006 9437.81419  0.97%   4.0  465s
 153329 24212 9476.13144   73   13 9530.24006 9438.56241  0.96%   4.0  470s
 155193 24206     cutoff   75      9530.24006 9439.25578  0.95%   4.0  475s
 157146 24191     cutoff   74      9530.24006 9440.09407  0.95%   4.0  480s
 159081 24136 9514.80649   79   13 9530.24006 9440.94313  0.94%   4.0  485s
 160949 24122     cutoff   77      9530.24006 9441.70554  0.93%   4.0  490s
 162923 24064 9509.04879   75   16 9530.24006 9442.54040  0.92%   4.0  495s
 164913 24042 9488.37211   73   13 9530.24006 9443.36104  0.91%   4.0  500s
 166881 24014 9528.37203   85    6 9530.24006 9444.25319  0.90%   4.0  505s
 168910 23987     cutoff   77      9530.24006 9445.02066  0.89%   4.0  510s
 170906 23951 9521.87924   77   13 9530.24006 9445.91675  0.88%   4.0  515s
 172775 23948 9499.69802   71   13 9530.24006 9446.76869  0.88%   4.1  520s
 174746 23907     cutoff   72      9530.24006 9447.49809  0.87%   4.1  525s
 176691 23912     cutoff   76      9530.24006 9448.40846  0.86%   4.1  530s
 178654 23861 9475.36718   72   15 9530.24006 9449.36078  0.85%   4.1  535s
 180708 23821 9476.32221   76   16 9530.24006 9450.31613  0.84%   4.1  540s
 182763 23706     cutoff   77      9530.24006 9451.20704  0.83%   4.1  545s
 184721 23652 9478.06550   71   15 9530.24006 9452.05605  0.82%   4.1  550s
 186577 23634 9522.73595   77   10 9530.24006 9452.82578  0.81%   4.1  555s
 188544 23585     cutoff   73      9530.24006 9453.58567  0.80%   4.1  560s
 190518 23539     cutoff   75      9530.24006 9454.51068  0.79%   4.1  565s
 192451 23444     cutoff   74      9530.24006 9455.34999  0.79%   4.1  570s
 194463 23382 9520.34820   84   11 9530.24006 9456.11182  0.78%   4.1  575s
 196464 23303     cutoff   83      9530.24006 9457.10910  0.77%   4.1  580s
 198323 23298     cutoff   70      9530.24006 9457.67323  0.76%   4.1  585s
 200339 23168     cutoff   72      9530.24006 9458.59537  0.75%   4.1  590s
 202321 23140     cutoff   76      9530.24006 9459.38439  0.74%   4.2  595s
 204401 23018 9488.90279   67    6 9530.24006 9460.34060  0.73%   4.2  600s
 206504 22885 9503.87637   73   13 9530.24006 9461.24563  0.72%   4.2  605s
 208465 22734 9507.40475   76   14 9530.24006 9462.12390  0.71%   4.2  610s
 210464 22627 9502.16368   82   13 9530.24006 9463.01002  0.71%   4.2  615s
 212421 22530     cutoff   79      9530.24006 9463.93965  0.70%   4.2  620s
 214461 22376     cutoff   83      9530.24006 9464.82542  0.69%   4.2  625s
 216428 22253 9516.56316   78   12 9530.24006 9465.65149  0.68%   4.2  630s
 218325 22138     cutoff   77      9530.24006 9466.43582  0.67%   4.2  635s
 220378 21923 9526.20034   74   12 9530.24006 9467.30235  0.66%   4.2  640s
 222430 21683     cutoff   74      9530.24006 9468.24172  0.65%   4.2  645s
 224507 21546 9520.19214   82   12 9530.24006 9468.99358  0.64%   4.2  650s
 226582 21357 9519.22287   83   10 9530.24006 9469.82754  0.63%   4.2  655s
 228674 21165 9508.64037   83   11 9530.24006 9470.66184  0.63%   4.2  660s
 230774 20989     cutoff   81      9530.24006 9471.45426  0.62%   4.2  665s
 232879 20756 9501.27395   75   12 9530.24006 9472.26082  0.61%   4.2  670s
 234991 20478 9494.51377   78   12 9530.24006 9473.11383  0.60%   4.2  675s
 237041 20306 9527.14056   79   10 9530.24006 9473.75288  0.59%   4.2  680s
 239104 20133 9498.17968   70   10 9530.24006 9474.58299  0.58%   4.2  685s
 240962 19931 9516.47955   71   12 9530.24006 9475.28006  0.58%   4.2  690s
 243065 19644     cutoff   75      9530.24006 9476.21259  0.57%   4.2  695s
 245141 19354 9517.99639   79    9 9530.24006 9477.06618  0.56%   4.2  700s
 247202 19031     cutoff   76      9530.24006 9477.92313  0.55%   4.2  705s
 249165 18704 9494.08316   72   14 9530.24006 9478.71121  0.54%   4.2  710s
 251226 18425 9523.23964   78   10 9530.24006 9479.54656  0.53%   4.2  715s
 253307 18034 9503.05024   77   18 9530.24006 9480.50417  0.52%   4.2  720s
 255424 17621 9508.69739   74   11 9530.24006 9481.38521  0.51%   4.2  725s
 257547 17148     cutoff   84      9530.24006 9482.43875  0.50%   4.2  730s
 259667 16630     cutoff   75      9530.24006 9483.57598  0.49%   4.2  735s
 261816 16043     cutoff   77      9530.24006 9484.74836  0.48%   4.2  740s
 263906 15401     cutoff   77      9530.24006 9485.92425  0.47%   4.3  745s
 265936 14815 9521.52958   76   13 9530.24006 9486.99728  0.45%   4.3  750s
 267848 14271     cutoff   78      9530.24006 9488.00311  0.44%   4.3  755s
 269967 13508     cutoff   75      9530.24006 9489.18178  0.43%   4.3  760s
 271956 12789     cutoff   78      9530.24006 9490.35436  0.42%   4.3  765s
 273938 11987 9508.94973   78   13 9530.24006 9491.52925  0.41%   4.3  770s
 275875 11226     cutoff   74      9530.24006 9492.59119  0.40%   4.3  775s
 277705 10364     cutoff   78      9530.24006 9494.04547  0.38%   4.3  780s
 279379  9626     cutoff   77      9530.24006 9495.37158  0.37%   4.3  785s
 281178  8797     cutoff   86      9530.24006 9497.06212  0.35%   4.3  790s
 283088  7887 9513.98032   73   14 9530.24006 9498.96515  0.33%   4.3  795s
 284949  7064 9527.47077   78   13 9530.24006 9501.07795  0.31%   4.3  800s
 286818  6303 9519.66281   82    8 9530.24006 9503.31579  0.28%   4.3  805s
 288675  5448     cutoff   85      9530.24006 9505.92944  0.26%   4.3  810s
 290397  4524     cutoff   76      9530.24006 9508.74706  0.23%   4.3  815s
 292288  3453 9528.29292   80   11 9530.24006 9512.53773  0.19%   4.3  820s
 294087  2274 9530.09563   82    4 9530.24006 9516.88295  0.14%   4.4  825s
 295996  1021 9527.24787   88    9 9530.24006 9522.93125  0.08%   4.4  830s
 297858   313 9528.99866   95    8 9530.24006 9528.80096  0.02%   4.4  835s
*298130   199             108    9530.2387908 9529.49201  0.01%   4.4  835s

Explored 298500 nodes (1312341 simplex iterations) in 836.62 seconds (285.48 work units)
Thread count was 1 (of 56 available processors)

Solution count 10: 9530.24 9530.25 9538.76 ... 9953.86

Optimal solution found (tolerance 1.00e-06)
Best objective 9.530238790845e+03, best bound 9.530238790845e+03, gap 0.0000%

User-callback calls 1086422, time in user-callback 0.69 sec
reading holmberg data file ../../../../../instances/uflp/holmberg/p27
primal_bound = 9530.238790845357, dual_bound = 9530.238790845355, gap = 1.9086503953008587e-14, time = 836.63
OPTIMAL LOCATIONS:
y[1] = 1.0
y[2] = 1.0
y[3] = 1.0
y[4] = 1.0
y[5] = 1.0
y[6] = 1.0
y[7] = 1.0
y[8] = 1.0
y[9] = 1.0
y[10] = 1.0
y[11] = 1.0
y[12] = 1.0
y[13] = 1.0
y[14] = 1.0
y[15] = 1.0
y[16] = 1.0
y[17] = 1.0
y[18] = 1.0
y[19] = 1.0
y[20] = 1.0
y[21] = 1.0
y[22] = 1.0
y[23] = 1.0
y[24] = 1.0
y[25] = 1.0
y[26] = 1.0
y[27] = 1.0
y[28] = 1.0
y[29] = 1.0
y[30] = 1.0
OPTIMAL WAREHOUSING:
f[1] = 146.0
f[3] = 187.0
f[6] = 335.0
f[9] = 242.0
f[12] = 433.0
f[15] = 329.0
f[16] = 20.0
f[18] = 399.0
f[21] = 451.0
f[25] = 428.0
OPTIMAL WAREHOUSING COSTS:
nlobj[1] = 110.8574840481079
nlobj[2] = 0.0
nlobj[3] = 125.46205312138179
nlobj[4] = 0.0
nlobj[5] = 0.0
nlobj[6] = 167.92447007574026
nlobj[7] = 0.0
nlobj[8] = 0.0
nlobj[9] = 142.72474081158893
nlobj[10] = 0.0
nlobj[11] = 0.0
nlobj[12] = 190.91290113971365
nlobj[13] = 0.0
nlobj[14] = 0.0
nlobj[15] = 166.41394910221183
nlobj[16] = 41.03044818794584
nlobj[17] = 0.0
nlobj[18] = 183.26439459213486
nlobj[19] = 0.0
nlobj[20] = 0.0
nlobj[21] = 194.84084072501656
nlobj[22] = 0.0
nlobj[23] = 0.0
nlobj[24] = 0.0
nlobj[25] = 189.80750904151392
nlobj[26] = 0.0
nlobj[27] = 0.0
nlobj[28] = 0.0
nlobj[29] = 0.0
nlobj[30] = 0.0
OPTIMAL ASSIGNMENTS:
x[1, 22] = 1.0
x[1, 41] = 1.0
x[1, 61] = 1.0
x[1, 68] = 1.0
x[1, 82] = 1.0
x[1, 126] = 1.0
x[1, 129] = 1.0
x[1, 131] = 1.0
x[1, 141] = 1.0
x[3, 7] = 1.0
x[3, 21] = 1.0
x[3, 27] = 1.0
x[3, 91] = 1.0
x[3, 97] = 1.0
x[3, 105] = 1.0
x[3, 113] = 1.0
x[3, 149] = 1.0
x[6, 6] = 1.0
x[6, 19] = 1.0
x[6, 23] = 1.0
x[6, 25] = 1.0
x[6, 29] = 1.0
x[6, 31] = 1.0
x[6, 51] = 1.0
x[6, 54] = 1.0
x[6, 62] = 1.0
x[6, 90] = 1.0
x[6, 106] = 1.0
x[6, 121] = 1.0
x[6, 127] = 1.0
x[6, 134] = 1.0
x[6, 138] = 1.0
x[6, 148] = 1.0
x[6, 150] = 1.0
x[9, 12] = 1.0
x[9, 13] = 1.0
x[9, 26] = 1.0
x[9, 44] = 1.0
x[9, 50] = 1.0
x[9, 76] = 1.0
x[9, 101] = 1.0
x[9, 110] = 1.0
x[9, 122] = 1.0
x[9, 125] = 1.0
x[9, 135] = 1.0
x[9, 142] = 1.0
x[9, 145] = 1.0
x[12, 2] = 1.0
x[12, 20] = 1.0
x[12, 32] = 1.0
x[12, 34] = 1.0
x[12, 39] = 1.0
x[12, 49] = 1.0
x[12, 58] = 1.0
x[12, 59] = 1.0
x[12, 64] = 1.0
x[12, 65] = 1.0
x[12, 66] = 1.0
x[12, 73] = 1.0
x[12, 94] = 1.0
x[12, 99] = 1.0
x[12, 104] = 1.0
x[12, 108] = 1.0
x[12, 114] = 1.0
x[12, 115] = 1.0
x[12, 116] = 1.0
x[12, 123] = 1.0
x[12, 130] = 1.0
x[12, 132] = 1.0
x[12, 146] = 1.0
x[12, 147] = 1.0
x[15, 4] = 1.0
x[15, 9] = 1.0
x[15, 16] = 1.0
x[15, 17] = 1.0
x[15, 40] = 1.0
x[15, 42] = 1.0
x[15, 46] = 1.0
x[15, 52] = 1.0
x[15, 53] = 1.0
x[15, 60] = 1.0
x[15, 79] = 1.0
x[15, 109] = 1.0
x[15, 111] = 1.0
x[15, 112] = 1.0
x[15, 136] = 1.0
x[15, 144] = 1.0
x[16, 43] = 1.0
x[18, 10] = 1.0
x[18, 15] = 1.0
x[18, 18] = 1.0
x[18, 24] = 1.0
x[18, 47] = 1.0
x[18, 57] = 1.0
x[18, 74] = 1.0
x[18, 84] = 1.0
x[18, 85] = 1.0
x[18, 86] = 1.0
x[18, 87] = 1.0
x[18, 88] = 1.0
x[18, 93] = 1.0
x[18, 95] = 1.0
x[18, 98] = 1.0
x[18, 102] = 1.0
x[18, 103] = 1.0
x[18, 107] = 1.0
x[18, 137] = 1.0
x[18, 140] = 1.0
x[21, 1] = 1.0
x[21, 3] = 1.0
x[21, 5] = 1.0
x[21, 14] = 1.0
x[21, 33] = 1.0
x[21, 35] = 1.0
x[21, 36] = 1.0
x[21, 48] = 1.0
x[21, 55] = 1.0
x[21, 56] = 1.0
x[21, 63] = 1.0
x[21, 71] = 1.0
x[21, 72] = 1.0
x[21, 78] = 1.0
x[21, 81] = 1.0
x[21, 83] = 1.0
x[21, 89] = 1.0
x[21, 100] = 1.0
x[21, 124] = 1.0
x[21, 139] = 1.0
x[21, 143] = 1.0
x[25, 8] = 1.0
x[25, 11] = 1.0
x[25, 28] = 1.0
x[25, 30] = 1.0
x[25, 37] = 1.0
x[25, 38] = 1.0
x[25, 45] = 1.0
x[25, 67] = 1.0
x[25, 69] = 1.0
x[25, 70] = 1.0
x[25, 75] = 1.0
x[25, 77] = 1.0
x[25, 80] = 1.0
x[25, 92] = 1.0
x[25, 96] = 1.0
x[25, 117] = 1.0
x[25, 118] = 1.0
x[25, 119] = 1.0
x[25, 120] = 1.0
x[25, 128] = 1.0
x[25, 133] = 1.0
fixed costs = 15000.0
assignment costs = 8017.0
warehousing costs = 1513.2387908453554
total cost = 9530.238790845355
